This file contains stuff Hogg has deleted from the main line.

\section{The plausibility}

ALL WRONG / FIX....students, especially undergraduates interested in majoring in physics.
It is also a question that is (effectively) asked by many popular
movies, including \textit{The Matrix} (DATE), \textit{Total Recall}
(DATE) and others.
It seems ridiculous to even contemplate it, but then---as one does
contemplate it---it becomes clear that it is hard to \emph{rule out}
empirically or observationally.
What would it be like to live in a simulation, and how would it be
different from---or the same as---what we currently observe?
The question has religious connotations or connections too, because if
it turns out to be true that we do live in a simulation, then it might
turn out to be true, strictly speaking, that we have a \emph{maker}.
As usual, however, when physics questions meet religious questions, it
will turn out
physics doesn't provide the relevant answers.

One of the remarkable things about physical law, at it's most fundamental,
is that if the world is unitary and Hamiltonian, then there is a very
true sense that we could be an a perfect analog-computer simulation.
In both classical mechanics (\eg, CITE GOLDSTEIN) and quantum mechanics
(\eg, CITE SOMETHING), there are perfect symplectic coordinate transformations
between Hamiltonian systems, such that any Hamiltonian system (possibly subject
to some weak assumptions like a time-independent form for the Hamiltonian)
can be transformed into any other Hamiltonian system, delivering identical
dynamics.

When these transformations are made, combinations of positions and
velocities from one coordinate system or representation become
positions in the other coordinate system, and different combinations
become velocities....
Initial conditions...
Entropy and etc...

... the point that this means that the question has to become a
statistical or probabilistic one.

\section{The implausibility}

(Introduce spacetime volume and mass-time ideas. Call out the point
that within general relativity there is a measure problem that might
make what's being said here technically wrong, but you know what I
mean (sort of?). Woah, maybe this could be used to define a measure??)

\section{The high probability}

Famously, Bostrom (2003) solves this implausibility argument by saying
that \emph{obviously} extremely intelligent and technologically
capable entities will make ``ancestor simulations'' that have the
property that they are very approximate, but self-correcting: If an
intelligent agent within the simulation discovers that they are in the
simulation, the simulation will be run back and re-started with that
part simulated at higher resolution (so the intelligent agent never
makes any such discovery). Very clever! It permits a technological entity
to make a simulation that is undetectable by any agent within it, without
requiring simulation hardware that is larger in spacetime volume than
the world it simulates.

Bostrom goes on to say that if it is natural for any very
technologically capable entities to make such simulations, and if
those simulations are rich enough that the agents within them will
become technologically capable enough to make such simulations, then
they will also make rich simulations, and so on. Once you build this
tower of simulations, he claims, we are exceedingly unlikely to be at
the top level of the tower.
That is, most intelligent creatures in the full stack
of universes (top-level plus all simulations) will end up \emph{not}
being in that top-level universe. Good argument. Or is it?

In addition to the usual eye-rolling that these arguments receive, I
would highlight two extremely weak assumptions:

The first weakness is that, ultimately, all of these simulated
universes are being simulated by the top-level hardware. That is,
every move that is made by every agent in every simulation within
every simulation within every simulation of this simulation stack is
being computed by the hardware at the top level. Even if this
top-level hardware is very sophisicated---even if the entities
operating it have commandeered the vast majority of the computing
available to them on their planetary system which itself is made
entirely of computers, there will have to be incredibly effective
speed-ups and approximations going on in the simulations to make this
all possible.

One way to think about it is this: Imagine that the simulations are
happening with an efficency factor of $q$: That is, because of amazing
speed-ups obtained by performing the simulations sufficiently
approximately, every tonne-hour of computing at the top level can
compute $q>1$ tonne-hours of world-simulation in the stack of
simulated worlds, on average. Then technologically-capable
civilizations that want to run such simulations will put far more
intelligent agents into the simulations and sub-simulations than exist
at the top-level world when (very roughly speaking) they apportion far
more than $1/q$ of their total world (like their homes, biosphere,
computers, land, vehicles, and so on) into performing such
simulations. That seems like a lot! Even if you got $q$ close to 100,
which seems implausible, would you really use 1 percent of your total
resources (and I don't just mean compute resources, I mean
\emph{total} resources including all equipment, energy, food,
personnel, and so on) on this project? A project that only addresses a
small fraction of all interesting non-monetizable science questions?
That's a pretty strong assumption.

Bostrom considers it obvious that enormously expensive simulation
efforts would happen at every level in the hierarchy of
simulations. But I don't think it would happen at \emph{any} level in
the hierarchy. Unless the singularity (\eg, Good 1966, or Chalmers 2010) leaves us with
machines-in-charge that are way more interested in pure science than
we are! That is, the fact that there is a hierarchy of simulations
does not escape the problem that each simulation of every action in
every simulated universe puts compute requirements (in some product of
computing mass times time or total computing energy, say) at the top
level. Those requirements are finite, and probably killing.

The second weakness I'd like to point out is the unexamined premise
that you could just stop the simulation and reverse it when someone
inside the simulation discovers that they are in a simulation! CITE
WITTGENSTEIN AND THOMPSON.

\section{My attitude, and implications}

Comments on Hinchliffe's Law.

Does Bostrom's paper already rule out the hypothesis? No, because it
is a bad paper.

Comments on the existence of God.

\section*{References}
\begin{trivlist}
\item Bostrom,~N., 2003, Are You Living in a Computer Simulation?,
  \textit{Philosophical Quarterly} \textbf{53} 211 243--255.
\item Chalmers,~D.~J., The Singularity: A philosophical analysis,
  \textit{Journal of Consciousness Studies} \textbf{17} 7--65.
\item Good,~I.~J., 1966. Speculations concerning the first ultraintelligent machine,
  in (F.~L.~Alt \& M.~Rubinoff, eds.) \textit{Advances in Computers} \textbf{6} 31--88.
\item \textit{Matrix, The}, HOGG DATE ETC.
\item Thompson, A., 2017, Scientists Can Now Read Your Thoughts With a Brain Scan,
  \textit{Popular Mechanics} 2017 June 27.
\item \textit{Total Recall}, HOGG DATE ETC.
\item Wikipedia editors, Betteridge's law of headlines,
  \url{https://en.wikipedia.org/wiki/Betteridge%27s_law_of_headlines}
    (accessed 2020 March 28)
\item Wittgenstein, L., 1958, \textit{The Blue and Brown Books},
  Basil Blackwell.
\end{trivlist}
